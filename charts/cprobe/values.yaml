replicaCount: 1
namespaceCreate: false

image:
  repository: flashcatcloud/cprobe
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""


podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 5858

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

# if true, cprobe will not write metrics to remote target
noWrite: false

plugins:
  mysql: true
  redis: true
  kafka: true
  blackbox: false
  json: false
  prometheus: true
  elasticsearch: true
  oracle: true
  mongodb: true
  whois: true
  postgres: true
  tomcat: true
  memcached: true
  consul: true

# todo components
components:
  prometheus: false
  grafana: false

# cprobe config
cprobe:
  config:
    global:
      extra_labels:
        region: bj
        zone: x
      metric_relabel_configs:
        - source_labels: [ __name__ ]
          regex: '(.*)'
          target_label: env
          replacement: 'production'
    writers:
      - url: http://prometheus:9090/api/v1/write
        extra_labels:
          from: 9090
        metric_relabel_configs:
          - source_labels: [__name__]
            regex: '(.*)'
            target_label: foo
            replacement: 'bar_${1}'
        concurrency: 10
        retry_times: 100
        retry_interval_millis: 3000
        basic_auth_user: ""
        basic_auth_pass: ""
        headers: []
        connect_timeout_millis: 500
        request_timeout_millis: 5000
        max_idle_conns_per_host: 2
        proxy_url: ""
        interface: ""
        # todo tls
#        tls_skip_verify: false
#        tls_ca: /etc/ssl/certs/ca-certificates.crt
#        tls_cert: /etc/ssl/certs/client.crt
#        tls_key: /etc/ssl/certs/client.key
#        tls_key_pwd: password
#        tls_server_name: prometheus
        tls_min_version: "1.2"
        tls_max_version: "1.3"

#      - url: http://127.0.0.1:9091/api/v1/write
#        extra_labels:
#          from: 9091

redis:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'redis'
    scrape_configs:
      - job_name: 'redis'
        static_configs:
          - targets:
              - 'redis-master:6379'
        scrape_rule_files:
          - 'rule.toml'

  cluster: |
    is_cluster = false

  rule: |
    user = ""
    password = ""

kafka:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'kafka'

    scrape_configs:
      - job_name: 'kafka'
        static_configs:
          - targets:
              - 'kafka:9092'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    [global]
    namespace = "kafka"

mysql:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'mysql'

    scrape_configs:
      - job_name: 'mysql'
        static_configs:
          - targets:
              - 'mysql:3306'
        scrape_rule_files:
          - 'rule_head.toml'
          - 'rule_coll.toml'
  rule_head: |
    [global]
    user = 'root'
    password = 'root'


  rule_coll: |
    [collect_global_status]
    enabled = true
    
    [collect_global_variables]
    enabled = true
    
    [collect_slave_status]
    enabled = true
    
    [collect_info_schema_innodb_cmp]
    enabled = true
    
    [collect_info_schema_innodb_cmpmem]
    enabled = true
    
    [collect_info_schema_query_response_time]
    enabled = true
    
    [collect_info_schema_processlist]
    enabled = true
    # Minimum time a thread must be in each state to be counted
    min_time = 0
    # Enable collecting the number of processes by user
    processes_by_user = true
    # Enable collecting the number of processes by host
    processes_by_host = true
    
    [collect_info_schema_tables]
    enabled = false
    # The list of databases to collect table stats for, or '*' for all
    databases = "*"
    
    [collect_info_schema_innodb_tablespaces]
    enabled = false
    
    [collect_info_schema_innodb_metrics]
    enabled = false
    
    [collect_info_schema_userstats]
    enabled = false
    
    [collect_info_schema_clientstats]
    enabled = false
    
    [collect_info_schema_tablestats]
    enabled = false
    
    [collect_info_schema_schemastats]
    enabled = false
    
    [collect_info_schema_replica_host]
    enabled = false
    
    [collect_mysql_user]
    enabled = false
    # Enable collecting user privileges from mysql.user
    collect_user_privileges = false
    
    [collect_auto_increment_columns]
    enabled = false
    
    [collect_binlog_size]
    enabled = false
    
    [collect_perf_schema_tableiowaits]
    enabled = false
    
    [collect_perf_schema_indexiowaits]
    enabled = false
    
    [collect_perf_schema_tablelocks]
    enabled = false
    
    [collect_perf_schema_eventsstatements]
    enabled = false
    # Limit the number of events statements digests by response time
    limit = 250
    # Limit how old the 'last_seen' events statements can be, in seconds
    timelimit = 86400
    # Maximum length of the normalized statement text
    digest_text_limit = 120
    
    [collect_perf_schema_eventsstatementssum]
    enabled = false
    
    [collect_perf_schema_eventswaits]
    enabled = false
    
    [collect_perf_schema_file_events]
    enabled = false
    
    [collect_perf_schema_file_instances]
    enabled = false
    # RegEx file_name filter for performance_schema.file_summary_by_instance
    filter = ".*"
    # Remove path prefix in performance_schema.file_summary_by_instance
    remove_prefix = "/var/lib/mysql/"
    
    [collect_perf_schema_memory_events]
    enabled = false
    # Remove instrument prefix in performance_schema.memory_summary_global_by_event_name
    remove_prefix = "memory/"
    
    [collect_perf_schema_replication_group_members]
    enabled = false
    
    [collect_perf_schema_replication_group_member_stats]
    enabled = false
    
    [collect_perf_schema_replication_applier_status_by_worker]
    enabled = false
    
    [collect_sys_user_summary]
    enabled = false
    
    [collect_engine_tokudb_status]
    enabled = false
    
    [collect_engine_innodb_status]
    enabled = false
    
    [collect_heartbeat]
    enabled = false
    # Database from where to collect heartbeat data
    database = "heartbeat"
    # Table from where to collect heartbeat data
    table = "heartbeat"
    # Use UTC for timestamps of the current server
    utc = true
    
    [collect_slave_hosts]
    enabled = false

mongodb:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'mongodb'

    scrape_configs:
      - job_name: 'standalone'
        static_configs:
          - targets:
              - mongodb:27017
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    user = ""
    password = ""
    # SetDirect specifies whether or not a direct connect should be made.
    # If set to true, the driver will only connect to the host provided 
    # in the URI and will not discover other hosts in the cluster. 
    # Direct connections are not valid if multiple hosts are specified or
    # an SRV URI is used.
    direct_connect = true
    
    # Connection timeout
    connect_timeout = "5s"
    
    # to get $collStats
    # e.g. ["db1.col1", "db2.col2"]
    collstats_colls = []
    
    # to get $indexStats
    # e.g. ["db1.col1", "db2.col2"]
    indexstats_colls = []
    
    # Enable collecting metrics from getDiagnosticData
    collect_diagnosticdata = true
    
    # Enable collecting metrics from replSetGetStatus
    collect_replicasetstatus = true
    
    # Enable collecting metrics from dbStats
    collect_dbstats = true
    
    # Enable collecting free space metrics from dbStats
    collect_dbstatsfreestorage = true
    
    # Enable collecting metrics from top admin command
    collect_topmetrics = true
    
    # Enable collecting metrics currentop admin command
    collect_currentopmetrics = true
    
    # Enable collecting metrics from $indexStats
    collect_indexstats = true
    
    # Enable collecting metrics from $collStats
    collect_collstats = true
    
    # Enable collecting metrics from profile
    collect_profile = true
    
    # Set time window for scrape slow queries.
    collect_profile_slowqueries_time_window_seconds = 30
    
    # Enable descending index name override to replace -1 with _DESC
    metrics_override_descending_index = false
    
    # Disable collstats, dbstats, topmetrics and indexstats collector if there are more than <n> collections. 0=No limit
    disable_collstats_if_collcount_more_than = 0
    
    # Enable autodiscover collections
    discovering_mode = true
    
    # Enable old mongodb-exporter compatible metrics
    compatible_mode = true


prometheus:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'prometheus'

    scrape_configs:
      - job_name: 'xxx'
        static_configs:
          - targets:
              - 'http://prometheus:8080/metrics'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    headers = []
    connect_timeout_millis = 500
    request_timeout_millis = 5000
    # for duplicate # HELP and metric name
    split_body = true

elasticsearch:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'elasticsearch'

    scrape_configs:
      - job_name: 'elasticsearch'
        static_configs:
          - targets:
              - http://elasticsearch:9200
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    username = ""
    password = ""
    apikey = ""
    timeout = "5s"
    
    # set to "*"      -> /_nodes/stats
    # set to "_local" -> /_nodes/_local/stats
    gather_node = "*"
    
    # e.g. cluster cluster_uuid version build_date build_hash
    gather_cluster_info = true
    
    # _cluster/settings
    gather_cluster_settings = false
    
    # /_snapshot and /_snapshot/*/_all
    gather_snapshots = false
    
    # /_all/_stats
    # querystring: ignore_unavailable=true
    gather_indices = false
    
    # querystring: ignore_unavailable=true&level=shards
    # only gahter node_shards_total by node
    gather_indices_shards = false
    
    # /_all/_settings
    gather_indices_settings = false
    
    # /_all/_mappings
    gather_indices_mappings = false
    
    # /_alias
    gather_indices_use_alias = true
    
    # /_ilm/status and /_all/_ilm/explain
    gather_ilm = false
    
    # /_slm/stats and /_slm/status
    gather_slm = false
    
    # /_data_stream/*/_stats
    gather_data_stream = false
    
    # Path to PEM file that contains trusted Certificate Authorities for the Elasticsearch connection.
    tls_ca = ""
    
    # Path to PEM file that contains the private key for client auth when connecting to Elasticsearch.
    tls_client_private_key = ""
    
    # Path to PEM file that contains the corresponding cert for the private key to connect to Elasticsearch.
    tls_client_cert = ""
    
    # Skip SSL verification when connecting to Elasticsearch.
    tls_skip_verify = false
    
    # Region for AWS elasticsearch
    aws_region = ""
    
    # Role ARN of an IAM role to assume.
    aws_role_arn = ""   


oracle:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'oracle'

    scrape_configs:
      - job_name: 'oracle'
        static_configs:
          - targets:
              - oracle:1521/xe # ip:port/service
        scrape_rule_files:
          - 'link.toml'
          - 'comm.toml'
  link: |
    [global]
    username = "system"
    password = "oracle"
    options = {}

  comm: |
    [[queries]]
    mesurement = "sessions"
    label_fields = [ "status", "type" ]
    # value= "Gauge metric with count of sessions by status and type."
    value_fields = [ "value" ]
    request = "SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type"

    [[queries]]
    mesurement = "resource"
    label_fields = [ "resource_name" ]
    # current_utilization= "Generic counter metric from v$resource_limit view in Oracle (current value)."
    # limit_value="Generic counter metric from v$resource_limit view in Oracle (UNLIMITED: -1)."
    value_fields = [ "current_utilization", "limit_value" ]
    request="SELECT resource_name,current_utilization,CASE WHEN TRIM(limit_value) LIKE 'UNLIMITED' THEN '-1' ELSE TRIM(limit_value) END as limit_value FROM v$resource_limit"

    [[queries]]
    mesurement = "asm_diskgroup"
    label_fields = [ "name" ]
    # total = "Total size of ASM disk group."
    # free = "Free space available on ASM disk group."
    value_fields = [ "total", "free" ]
    request = "SELECT name,total_mb*1024*1024 as total,free_mb*1024*1024 as free FROM v$asm_diskgroup_stat where exists (select 1 from v$datafile where name like '+%')"

    [[queries]]
    mesurement = "activity"
    # value="Generic counter metric from v$sysstat view in Oracle."
    value_fields = [ "value" ]
    metric_name_field = "name"
    request = "SELECT name, value FROM v$sysstat WHERE name IN ('parse count (total)', 'execute count', 'user commits', 'user rollbacks')"

    [[queries]]
    mesurement = "process"
    # count="Gauge metric with count of processes." 
    value_fields = [ "count" ]
    request = "SELECT COUNT(*) as count FROM v$process"

    [[queries]]
    mesurement = "wait_time"
    # value="Generic counter metric from v$waitclassmetric view in Oracle."
    value_fields = [ "value" ]
    metric_name_field= "wait_class"
    request = '''
    SELECT
      n.wait_class as wait_class,
      round(m.time_waited/m.INTSIZE_CSEC,3) as value
    FROM
      v$waitclassmetric  m, v$system_wait_class n
    WHERE
      m.wait_class_id=n.wait_class_id AND n.wait_class != 'Idle'
    '''

    [[queries]]
    mesurement = "tablespace"
    label_fields = [ "tablespace", "type" ]
    # bytes = "Generic counter metric of tablespaces bytes in Oracle."
    # max_bytes = "Generic counter metric of tablespaces max bytes in Oracle."
    # free = "Generic counter metric of tablespaces free bytes in Oracle."
    # used_percent = "Gauge metric showing as a percentage of how much of the tablespace has been used."
    value_fields = [ "bytes", "max_bytes", "free", "used_percent" ]
    request = '''
    SELECT
        dt.tablespace_name as tablespace,
        dt.contents as type,
        dt.block_size * dtum.used_space as bytes,
        dt.block_size * dtum.tablespace_size as max_bytes,
        dt.block_size * (dtum.tablespace_size - dtum.used_space) as free,
        dtum.used_percent
    FROM  dba_tablespace_usage_metrics dtum, dba_tablespaces dt
    WHERE dtum.tablespace_name = dt.tablespace_name
    ORDER by tablespace
    '''

    [[queries]]
    mesurement = "slow_queries"
    # p95_time_usecs= "Gauge metric with percentile 95 of elapsed time.", p99_time_usecs= "Gauge metric with percentile 99 of elapsed time."
    value_fields = [ "p95_time_usecs", "p99_time_usecs" ]
    request = '''
    select percentile_disc(0.95) within group (order by elapsed_time) as p95_time_usecs,
      percentile_disc(0.99) within group (order by elapsed_time) as p99_time_usecs
    from v$sql where last_active_time >= sysdate - 5/(24*60)
    '''

    [[metrics]]
    mesurement = "sysmetric"
    value_fields = [ "value" ]
    metric_name_field = "metric_name"
    timeout = "3s"
    request = '''
    select METRIC_NAME,VALUE from v$sysmetric where group_id=2
    '''

    [[metrics]]
    mesurement = "archivelog"
    value_fields = [ "count" ]
    timeout = "3s"
    request = '''
    select count(*) as count from v$log_history where first_time>=to_date(sysdate)
    '''


whois:
  config:
    global:
      scrape_interval: 60s
      external_labels:
        cplugin: 'whois'

    scrape_configs:
      - job_name: 'domain'
        static_configs:
          - targets:
              - baidu.com
              - flashcat.cloud
  rule: |
    [global]

postgres:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'postgres'

    scrape_configs:
      - job_name: 'postgres'
        static_configs:
          - targets:
              - 'postgres:15432'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    # Authentication credentials
    username = "postgres"
    password = "password"
    options = { sslmode = "disable" }
    
    # Do not include default metrics
    disable_default_metrics = false
    
    # Do not include pg_settings metrics
    disable_settings_metrics = false
    
    enabled_collectors = [
    "database",
    "stat_database",
    "locks",
    "replication_slot",
    "replication",
    "stat_bgwriter",
    "stat_user_tables",
    "statio_user_tables",
    "wal"
    ]
tomcat:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'tomcat'

    scrape_configs:
      - job_name: 'tomcat'
        static_configs:
          - targets:
              - 'tomcat:8080'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    basic_auth_user = "tomcat"
    basic_auth_pass = "s3cret"
    # gather_servlets = false

memcached:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'memcached'

    scrape_configs:
      - job_name: 'memcached'
        static_configs:
          - targets:
              - 'memcached:11211'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    timeout = "1s"

consul:
  config:
    global:
      scrape_interval: 15s
      external_labels:
        cplugin: 'consul'

    scrape_configs:
      - job_name: 'consul'
        static_configs:
          - targets:
              - 'consul:8500'
        scrape_rule_files:
          - 'rule.toml'

  rule: |
    allow_stale = true